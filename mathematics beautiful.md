### 数学之美 ###
#### 系列一 统计语言模型 ####
数学是解决信息检索和自然语言处理的最好工具。

只要数一数这对词（wi-1,wi）在统计的文本中出现了多少次，以及wi-1本身在同一的文本中前后相邻出现了多少次，然后用两个数移一除就可以了，P(wi|wi-1)=P(wi|wi-1)/P(wi-1)

#### 系列二 谈谈中文分词 ####
- 查字典
	- 把一个句子从左向右扫描一遍，遇到字典里面有的词就标记出来，遇到复合词，就找最长的词匹配，遇到不认识的词串就分割成单字词。
	- 缺陷：二义性分割
- 不同应用场景的，汉语分词的颗粒度大小不同。

#### 系列三 隐含马尔可夫模型在语言处理中的应用 ####
- 马尔可夫模型
	1. s1,s2,s3,...是一个马尔科夫链，也就是说， si只由si-1决定
	2. 第i时刻的接收信号oi只由发送信号si决定。

#### 系列四 怎样度量信息 ####
- 香农的信息熵
	- 用“比特”度量信息量
	- 变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大。
- 一条信息的信息量大小和它的不确定性有直接的关系。

#### 系列五 简单之美：布尔代数和搜索引擎的索引 ####
- 早期的文献检索查询系统多数基于数据库，严格要求查询语句符合布尔运算。
- 今天的搜索引擎，自动把用户的查询语句转换成布尔运算的算式。
- 最简单索引的结构是用一个很长的二进制数便是一个关键字是否出现在每篇文献中。有多少篇文章，就有多少位数，每一位对应一篇文章，1代表相应的文献有这个关键字，0代表没有。

#### 系列六 图论和网络爬虫 ####
- 广度优先算法BFS
- 深度优先算法DFS
- 在网络爬虫中，我们使用一个“哈希表”的列表记录网页是否下载过的信息

#### 系列七 信息论在信息处理中的应用 ####
- 语言模型复杂度
	- 衡量语言模型的好坏
	- 一个模型的复杂度越小，模型越好
- 互信息
	- 对两个随机事件相关性的度量
	- 解决二义性的方法
- 相对熵（交叉熵）
	- 衡量两个正函数是否相似
	- 两个完全相同的函数，它们的相对熵等于零
	- 衡量两个常用词（在语法上河语义上）是否同义
	- 两篇文章的内容是否相近
	- 词频率-逆向文档频率（TF/IDF）

#### 系列八 贾里尼克的故事和现代语言处理 ####
- 贾里尼克的统计语音识别的框架结构
	- 把它当成通信问题
	- 并用两个隐含马尔可夫模型（声学模型和语言模型）概括语音识别
- 通信模型
	1. 上下文
	2. 信息
	3. 发送者
	4. 接收者
	5. 信道
	6. 编码

#### 系列九 如何确定网页和查询的相关性 ####
- 分词得关键字
- 关键词的频率（单文本词汇频率）
	- 关键字的次数进行归一化，就是用关键字的次数除以网页的总字数
- 应删除词
	- 在度量相关性是不应考虑它们的频率
- 权重
	- 一个词预测主题能力越强，权重就越大，反之，权重就越小。
	- 应删除词的权重应该是零
	- 在信息检索中，使用最多的权重是“逆文本频率指数IDF”
		- log（D/Dw） D是全部网页数

#### 系列十 有限状态机和地址识别 ####
- 有限状态机
	- 一个特殊的有向图，包括一些状态（节点）和连接这些状态的有向弧
- 基于概率的有限状态机和离散的马尔可夫链
	- 进行模糊匹配，并给出一个字串为正确地址的可能性

#### 系列十一 google阿卡47的制造者阿米特.辛格博士 ####
- 在计算机中一个好的算法，应该像阿卡47冲锋枪那样简单、有效、可靠性好，而且容易读懂（或者易操作）

#### 系列十二 余弦定理和新闻的分类 ####
- TF/IDF排序
- 如果两个向量的方向一致，即夹角接近零，那么这两个向量就相近
- 余弦定理
	- 确定两个向量方向是否一致
